{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78a31104",
   "metadata": {},
   "source": [
    "## 1. Check GPU and Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479881b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ” Checking GPU Availability\")\n",
    "print(\"=\"*70)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸  WARNING: No GPU detected! Training will be very slow.\")\n",
    "    print(\"Go to Runtime > Change runtime type > Select GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a6f0c0",
   "metadata": {},
   "source": [
    "## 2. Clone Repository and Navigate to Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f0dd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone your repository\n",
    "!git clone https://github.com/Zinoug/codeLLM-python-finetuning.git\n",
    "%cd codeLLM-python-finetuning\n",
    "\n",
    "# List files to confirm\n",
    "!ls -lh final_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f358c6bf",
   "metadata": {},
   "source": [
    "## 3. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e418ea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets evaluate rouge-score sacrebleu tqdm\n",
    "\n",
    "print(\"âœ… Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb9e5a0",
   "metadata": {},
   "source": [
    "## 4. Verify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f93aa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "print(\"ðŸ“Š Dataset Summary:\\n\")\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    filepath = f'final_data/{split}.jsonl'\n",
    "    tasks = []\n",
    "    \n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            sample = json.loads(line)\n",
    "            tasks.append(sample['task'])\n",
    "    \n",
    "    counts = Counter(tasks)\n",
    "    print(f\"{split.upper()}.jsonl: {sum(counts.values())} samples\")\n",
    "    for task, count in sorted(counts.items()):\n",
    "        print(f\"  â€¢ {task}: {count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37021fef",
   "metadata": {},
   "source": [
    "## 5. Fine-tune Model\n",
    "\n",
    "**Note:** Training will take approximately 2-3 hours on a T4 GPU.\n",
    "\n",
    "If you get **Out of Memory** errors:\n",
    "- Reduce `per_device_train_batch_size` to 1\n",
    "- Increase `gradient_accumulation_steps` to 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36dc0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the fine-tuning script\n",
    "!python finetune_codet5_multitask.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f6b119",
   "metadata": {},
   "source": [
    "## 6. Download Trained Model\n",
    "\n",
    "After training completes, download your model to use locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8651b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress the final model\n",
    "!tar -czf codet5_multitask_final.tar.gz codet5_multitask_final/\n",
    "\n",
    "print(\"âœ… Model compressed!\")\n",
    "print(\"\\nðŸ“¥ Download the model:\")\n",
    "\n",
    "from google.colab import files\n",
    "files.download('codet5_multitask_final.tar.gz')\n",
    "\n",
    "print(\"\\nðŸ’¡ To use locally, extract with: tar -xzf codet5_multitask_final.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de0b41a",
   "metadata": {},
   "source": [
    "## 7. (Optional) Quick Evaluation\n",
    "\n",
    "Test a few samples to verify the model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88ec0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(\"Loading model for quick test...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codet5_multitask_final\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"codet5_multitask_final\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "def predict(text, max_length=128):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_length, num_beams=4)\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test examples\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ§ª Quick Tests\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test 1: Code Repair\n",
    "buggy = \"\"\"fix bug:\n",
    "def add(a, b):\n",
    "    return a - b\"\"\"\n",
    "print(\"\\n1. Code Repair:\")\n",
    "print(f\"Input: {buggy}\")\n",
    "print(f\"Output: {predict(buggy)}\")\n",
    "\n",
    "# Test 2: Bug Detection\n",
    "code = \"\"\"classify code:\n",
    "def add(a, b):\n",
    "    return a + b\"\"\"\n",
    "print(\"\\n2. Bug Detection:\")\n",
    "print(f\"Input: {code}\")\n",
    "print(f\"Output: {predict(code, max_length=10)}\")\n",
    "\n",
    "# Test 3: Code Summary\n",
    "func = \"\"\"summarize code:\n",
    "def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fibonacci(n-1) + fibonacci(n-2)\"\"\"\n",
    "print(\"\\n3. Code Summary:\")\n",
    "print(f\"Input: {func}\")\n",
    "print(f\"Output: {predict(func, max_length=64)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ¨ Model is working!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66cbe39",
   "metadata": {},
   "source": [
    "## 8. (Optional) Mount Google Drive\n",
    "\n",
    "Save model directly to Google Drive to avoid re-downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a87db65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copy model to Drive\n",
    "!cp -r codet5_multitask_final /content/drive/MyDrive/\n",
    "\n",
    "print(\"âœ… Model saved to Google Drive: MyDrive/codet5_multitask_final/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b124218",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“‹ Troubleshooting\n",
    "\n",
    "### Out of Memory Error\n",
    "Edit `finetune_codet5_multitask.py` and reduce batch size:\n",
    "```python\n",
    "per_device_train_batch_size=1,  # Reduce from 2 to 1\n",
    "gradient_accumulation_steps=8,  # Increase from 4 to 8\n",
    "```\n",
    "\n",
    "### Session Timeout\n",
    "Colab disconnects after 12 hours. To continue training:\n",
    "1. The script saves checkpoints every 500 steps\n",
    "2. Edit `finetune_codet5_multitask.py` to resume from checkpoint:\n",
    "```python\n",
    "trainer.train(resume_from_checkpoint=\"codet5_multitask_checkpoint/checkpoint-500\")\n",
    "```\n",
    "\n",
    "### Slow Training\n",
    "- Make sure GPU is enabled (Runtime > Change runtime type)\n",
    "- T4 GPU: ~2-3 hours\n",
    "- CPU: ~20+ hours (not recommended)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Next Steps\n",
    "\n",
    "After training:\n",
    "1. Download model with cell #6\n",
    "2. Run evaluation locally: `python evaluate_model.py`\n",
    "3. Check pass@1 score for code repair task\n",
    "\n",
    "---\n",
    "\n",
    "**Academic Project** - Seoul National University of Technology and Science"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
